{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d60648a-f3cf-40a6-8871-07509fa8acfd",
   "metadata": {},
   "source": [
    "# Guitar Chord Model Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418eef9f-6615-42df-a695-e961885f4f31",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6580f308-2176-461f-95ef-cd1509ba639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import sys\n",
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4a232-4934-4190-867e-ca0276bbc1a0",
   "metadata": {},
   "source": [
    "### Load the Model and Setup the Categorical Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a298a97-1523-4438-949a-279bc226708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE MODEL HERE\n",
    "model = load_model('cnn_attempt_98_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d465b9f-171f-4641-b90e-b7cc7610b862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'am' 'bm' 'c' 'd' 'dm' 'e' 'em' 'f' 'g']\n"
     ]
    }
   ],
   "source": [
    "# SETUP THE CHORD LABELS\n",
    "label_encoder = LabelEncoder()\n",
    "chords_list = ['a', 'am', 'bm', 'c', 'd', 'dm', 'e', 'em', 'f', 'g']\n",
    "chords_array = np.array(chords_list)\n",
    "y = label_encoder.fit_transform(chords_array)\n",
    "# PRINT THE CHORD OPTIONS\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e80012-3aec-4a9e-8979-6001c0cdeaeb",
   "metadata": {},
   "source": [
    "### Setup an Audio Recorder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe115ba5-a755-42ef-bb84-8502e7e4a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP AN AUDIO RECORDER\n",
    "class AudioRecorder:\n",
    "    def __init__(self, filename='recording.wav', chunk=1024, channels=1, rate=44100, threshold=2000, duration=2):\n",
    "        self.filename = filename\n",
    "        self.chunk = chunk\n",
    "        self.channels = channels\n",
    "        self.rate = rate\n",
    "        self.frames = []\n",
    "        self.threshold = threshold\n",
    "        self.duration = duration\n",
    "        self.pa = pyaudio.PyAudio()\n",
    "        self.stream = self.pa.open(format=pyaudio.paInt16,\n",
    "                                   channels=self.channels,\n",
    "                                   rate=self.rate,\n",
    "                                   input=True,\n",
    "                                   frames_per_buffer=self.chunk)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def start_recording(self):\n",
    "        while True:\n",
    "            print(\"Listening for threshold level...\")\n",
    "            while True:\n",
    "                data = np.frombuffer(self.stream.read(self.chunk), dtype=np.int16)\n",
    "                if np.max(data) > self.threshold:\n",
    "                    print(\"Threshold level reached. Recording started...\")\n",
    "                    self.record()\n",
    "                    break\n",
    "\n",
    "    def record(self):\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < self.duration:\n",
    "            data = self.stream.read(self.chunk)\n",
    "            self.lock.acquire()\n",
    "            self.frames.append(data)\n",
    "            self.lock.release()\n",
    "        self.stop_recording()\n",
    "        self.frames.clear()\n",
    "        time.sleep(0.5)  # Wait for a moment before restarting recording\n",
    "\n",
    "    def stop_recording(self):\n",
    "        print(\"Recording stopped...\")\n",
    "        with wave.open(self.filename, 'wb') as wf:\n",
    "            wf.setnchannels(self.channels)\n",
    "            wf.setsampwidth(self.pa.get_sample_size(pyaudio.paInt16))\n",
    "            wf.setframerate(self.rate)\n",
    "            self.lock.acquire()\n",
    "            wf.writeframes(b''.join(self.frames))\n",
    "            self.lock.release()\n",
    "        print(f\"Recording saved as {self.filename}\")\n",
    "        self.predict(self.filename)\n",
    "\n",
    "    def predict(self, filename):\n",
    "        # LOAD THE AUDIO FILE\n",
    "        audio_file = filename  # Use the recorded WAV file\n",
    "        audio_sample, sr = librosa.load(audio_file)\n",
    "    \n",
    "        # PREPROCESS THE FEATURES OF THE AUDIO FILE\n",
    "        features = generate_features(audio_sample)\n",
    "    \n",
    "        # RESHAPE THE ARRAY TO MATCH THE EXPECTED INPUT SIZE OF THE MODEL\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "        # MAKE PREDICTION\n",
    "        predictions = model.predict(features)\n",
    "    \n",
    "        # GET THE PREDICTED CLASS\n",
    "        predicted_class = np.argmax(predictions)\n",
    "\n",
    "        # TRANSFORM IT INTO THE CHORD NAME\n",
    "        predicted_categorical_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "        # predicted_categorical_label HAS THE PREDICTED CHORD TYPE <- DISPLAY THIS ON THE PAGE\n",
    "    \n",
    "        # PRINT THE CHORD NAME\n",
    "        print(f\"For audio file {audio_file}, predicted class: {predicted_categorical_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fef97-880e-4fc7-9649-d4b932258b11",
   "metadata": {},
   "source": [
    "### Functions to Process the Live Recorded Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0220f1d-c358-44df-95bf-2aea7e46e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PADS THE FEATURES IF THEY ARE SMALLER THAN THE EXPECTED SHAPE\n",
    "def padding(array, xx, yy):\n",
    "    \"\"\"\n",
    "    :param array: numpy array\n",
    "    :param xx: desired height\n",
    "    :param yy: desirex width\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "    a = max((xx - h) // 2,0)\n",
    "    aa = max(0,xx - a - h)\n",
    "    b = max(0,(yy - w) // 2)\n",
    "    bb = max(yy - b - w,0)\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n",
    "\n",
    "# FUNCTION TO NORMALIZE A FEATURE\n",
    "def normalize(feature):\n",
    "    return (feature - np.min(feature)) / (np.max(feature) - np.min(feature))\n",
    "\n",
    "# EXTRACTS THE NEEDED FEATURES FROM AN AUDIO FILE\n",
    "def generate_features(audio_sample):\n",
    "    max_size = 1000\n",
    "    n_mfcc = 13\n",
    "    sr = 22050\n",
    "\n",
    "    # EXTRACT STFT, MFCCS, SPECTRAL CENTROID, CHROMA STFT, AND SPECTRAL BANDWIDTH\n",
    "    stft = librosa.stft(y=audio_sample, n_fft=255, hop_length=512)\n",
    "    stft = stft[:, :max_size]  # Truncate stft to max_size\n",
    "    stft = padding(np.abs(stft), 128, max_size)\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_sample, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs = mfccs[:, :max_size]  # Truncate mfccs to max_size\n",
    "    mfccs = padding(mfccs, 128, max_size)\n",
    "\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=audio_sample, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio_sample, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=audio_sample, sr=sr)\n",
    "\n",
    "    # TRUNCATE THE SIZE OF A FEATURE IF IT EXCEEDS MAX_SIZE (NEEDED FOR COMPUTATIONAL EFFICIENCY)\n",
    "    spec_bw_truncated = spec_bw[:, :max_size]\n",
    "    spec_centroid_truncated = spec_centroid[:, :max_size]\n",
    "    chroma_stft_truncated = chroma_stft[:, :max_size]\n",
    "\n",
    "    # CREATE THE IMAGE STACK\n",
    "    image = np.array([padding(normalize(spec_bw_truncated), 1, max_size)]).reshape(1, max_size)\n",
    "    image = np.append(image, padding(normalize(spec_centroid_truncated), 1, max_size), axis=0)\n",
    "\n",
    "    # THESE THREE FEATURES ARE BEING STACKED INTO ONE DIMENSION\n",
    "    for i in range(0, 9):\n",
    "        image = np.append(image, padding(normalize(spec_bw_truncated), 1, max_size), axis=0)\n",
    "        image = np.append(image, padding(normalize(spec_centroid_truncated), 1, max_size), axis=0)\n",
    "        image = np.append(image, padding(normalize(chroma_stft_truncated), 12, max_size), axis=0)\n",
    "\n",
    "    # STACK STFT AND MFCCS INTO THEIR OWN DIMENSION\n",
    "    image = np.dstack((image, stft))\n",
    "    image = np.dstack((image, mfccs))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf7fa7-76ea-4c90-bf14-c5337e5dbb5c",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae462d7-f525-40d8-aee9-5bbf68a3a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "For audio file recording.wav, predicted class: e\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "For audio file recording.wav, predicted class: am\n",
      "Listening for threshold level...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filename = 'recording.wav'\n",
    "    recorder = AudioRecorder(filename)\n",
    "\n",
    "    # START A LIVE RECORDING FEED\n",
    "    recorder.start_recording()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e09ac-980d-461d-b35a-62c0a67f1e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
