{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f5f0707-bd57-4d79-9808-e74da4bb77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyaudio\n",
    "import queue\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "import time\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04a1c39-2aea-4475-94af-03721a4cba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'am' 'bm' 'c' 'd' 'dm' 'e' 'em' 'f' 'g']\n",
      "Device 0: Microsoft Sound Mapper - Input\n",
      "Device 1: Microphone (Yeti Stereo Microph\n",
      "Device 2: Microsoft Sound Mapper - Output\n",
      "Device 3: Speakers (Conexant ISST Audio)\n",
      "Device 4: Speakers (Yeti Stereo Microphon\n",
      "Device 5: Primary Sound Capture Driver\n",
      "Device 6: Microphone (Yeti Stereo Microphone)\n",
      "Device 7: Primary Sound Driver\n",
      "Device 8: Speakers (Conexant ISST Audio)\n",
      "Device 9: Speakers (Yeti Stereo Microphone)\n",
      "Device 10: Speakers (Yeti Stereo Microphone)\n",
      "Device 11: Speakers (Conexant ISST Audio)\n",
      "Device 12: Microphone (Yeti Stereo Microphone)\n",
      "Device 13: Output 1 (Conexant ISST Audio output)\n",
      "Device 14: Output 2 (Conexant ISST Audio output)\n",
      "Device 15: Input (Conexant ISST Audio output)\n",
      "Device 16: Stereo Mix (Conexant ISST Stereo Mix)\n",
      "Device 17: Microphone Array (Conexant ISST Audio capture)\n",
      "Device 18: Speakers (Yeti Stereo Microphone)\n",
      "Device 19: Microphone (Yeti Stereo Microphone)\n",
      "Default Input Device:\n",
      "  Name: Microphone (Yeti Stereo Microph\n",
      "  Index: 1\n",
      "  Channels: 2\n",
      "  Default Sample Rate: 44100.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Given list of strings\n",
    "chords_list = ['a', 'am', 'bm', 'c', 'd', 'dm', 'e', 'em', 'f', 'g']\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "chords_array = np.array(chords_list)\n",
    "\n",
    "y = label_encoder.fit_transform(chords_array)\n",
    "\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Get the number of available audio devices\n",
    "p = pyaudio.PyAudio()\n",
    "device_count = p.get_device_count()\n",
    "\n",
    "# Print information about each audio device\n",
    "for i in range(device_count):\n",
    "    device_info = p.get_device_info_by_index(i)\n",
    "    print(f\"Device {i}: {device_info['name']}\")\n",
    "\n",
    "# Specify the index of the microphone you want to use\n",
    "# Replace 'mic_index' with the index of your microphone\n",
    "mic_index = 0  # Change this to the index of your microphone\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Get the index of the default input device\n",
    "default_input_device_index = p.get_default_input_device_info()['index']\n",
    "\n",
    "# Get information about the default input device\n",
    "default_input_device_info = p.get_device_info_by_index(default_input_device_index)\n",
    "\n",
    "# Print information about the default input device\n",
    "print(\"Default Input Device:\")\n",
    "print(f\"  Name: {default_input_device_info['name']}\")\n",
    "print(f\"  Index: {default_input_device_info['index']}\")\n",
    "print(f\"  Channels: {default_input_device_info['maxInputChannels']}\")\n",
    "print(f\"  Default Sample Rate: {default_input_device_info['defaultSampleRate']} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f65f8e3b-3b10-438b-9f74-ab4973375fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Captured audio saved as: captured_audio_0.wav\n",
      "WARNING:tensorflow:6 out of the last 24 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002E384E59760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 24 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002E384E59760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "Predicted chord: bm\n",
      "Stream stopped.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = load_model('cnn_attempt_98_acc.h5')\n",
    "\n",
    "# Define function to preprocess audio data\n",
    "def preprocess_audio(audio_sample):\n",
    "    # Generate features\n",
    "    features = generate_features(audio_sample)\n",
    "    # Reshape the feature array to match the input shape of the model\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "    return features\n",
    "\n",
    "def padding(array, xx, yy):\n",
    "    \"\"\"\n",
    "    :param array: numpy array\n",
    "    :param xx: desired height\n",
    "    :param yy: desirex width\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "    a = max((xx - h) // 2,0)\n",
    "    aa = max(0,xx - a - h)\n",
    "    b = max(0,(yy - w) // 2)\n",
    "    bb = max(yy - b - w,0)\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n",
    "\n",
    "# Function to generate features (replace with your own implementation)\n",
    "def generate_features(audio_sample, sr=44100):\n",
    "    max_size = 1000  # Define your max audio feature width\n",
    "    n_mfcc = 13  # Number of MFCC coefficients\n",
    "\n",
    "    # Extract features\n",
    "    stft = librosa.stft(y=audio_sample, n_fft=255, hop_length=512)\n",
    "    stft = stft[:, :max_size]  # Truncate stft to max_size\n",
    "    stft = padding(np.abs(stft), 128, max_size)\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_sample, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs = mfccs[:, :max_size]  # Truncate mfccs to max_size\n",
    "    mfccs = padding(mfccs, 128, max_size)\n",
    "\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=audio_sample, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio_sample, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=audio_sample, sr=sr)\n",
    "\n",
    "    spec_bw_truncated = spec_bw[:, :max_size]  # Truncate spec_bw to max_size\n",
    "    spec_centroid_truncated = spec_centroid[:, :max_size]  # Truncate spec_centroid to max_size\n",
    "    chroma_stft_truncated = chroma_stft[:, :max_size]\n",
    "\n",
    "    # Create the image stack\n",
    "    image = np.array([padding(normalize(spec_bw_truncated), 1, max_size)]).reshape(1, max_size)\n",
    "    image = np.append(image, padding(normalize(spec_centroid_truncated), 1, max_size), axis=0)\n",
    "\n",
    "    for i in range(0, 9):\n",
    "        image = np.append(image, padding(normalize(spec_bw_truncated), 1, max_size), axis=0)\n",
    "        image = np.append(image, padding(normalize(spec_centroid_truncated), 1, max_size), axis=0)\n",
    "        image = np.append(image, padding(normalize(chroma_stft_truncated), 12, max_size), axis=0)\n",
    "\n",
    "    # Stack STFT and MFCCs\n",
    "    image = np.dstack((image, stft))\n",
    "    image = np.dstack((image, mfccs))\n",
    "\n",
    "    return image\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_chord(features):\n",
    "    predictions = model.predict(features)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    predicted_categorical_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return predicted_categorical_label\n",
    "\n",
    "# Global variables to control listening duration and buffer\n",
    "LISTEN_DURATION = 5  # Duration to listen in seconds\n",
    "buffer = []\n",
    "start_time = time.time()  # Initialize start_time\n",
    "\n",
    "# Global variable to keep track of the file index\n",
    "file_index = 0\n",
    "# Callback function for PyAudio\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global buffer, start_time, file_index\n",
    "    \n",
    "    # Convert byte data to numpy array and normalize to the range [-1, 1]\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16) / 32767.0\n",
    "    \n",
    "    # Calculate the root mean square (RMS) amplitude of the audio signal\n",
    "    rms_amplitude = np.sqrt(np.mean(np.square(audio_data)))\n",
    "    \n",
    "    # Define a threshold value for sound detection\n",
    "    threshold = 0.1  # Adjust this threshold as needed\n",
    "    \n",
    "    # If RMS amplitude exceeds the threshold, capture audio\n",
    "    if rms_amplitude > threshold:\n",
    "        buffer.append(audio_data)\n",
    "    \n",
    "    # Check if the listening duration has elapsed\n",
    "    if time.time() - start_time >= LISTEN_DURATION:\n",
    "        if buffer:\n",
    "            # Concatenate audio data from buffer\n",
    "            audio_data = np.concatenate(buffer)\n",
    "            \n",
    "            # Save the captured audio data to a WAV file\n",
    "            filename = f\"captured_audio_{file_index}.wav\"\n",
    "            with wave.open(filename, 'wb') as wf:\n",
    "                wf.setnchannels(channels)\n",
    "                wf.setsampwidth(p.get_sample_size(audio_format))\n",
    "                wf.setframerate(sample_rate)\n",
    "                wf.writeframes(audio_data.tobytes())\n",
    "            print(f\"Captured audio saved as: {filename}\")\n",
    "            file_index += 1\n",
    "            \n",
    "            # Preprocess audio\n",
    "            features = preprocess_audio(audio_data)\n",
    "            \n",
    "            # Make prediction\n",
    "            predicted_chord = predict_chord(features)\n",
    "            print(f\"Predicted chord: {predicted_chord}\")\n",
    "        \n",
    "        # Reset buffer and start time\n",
    "        buffer = []\n",
    "        start_time = time.time()\n",
    "    \n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "# Set up PyAudio\n",
    "audio_format = pyaudio.paInt16  # Set to 16-bit integer format\n",
    "channels = 1  # Mono\n",
    "sample_rate = 44100  # 44100 Hz sample rate\n",
    "chunk_size = 1024\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=audio_format,\n",
    "                channels=channels,\n",
    "                rate=sample_rate,\n",
    "                input=True,\n",
    "                frames_per_buffer=chunk_size,\n",
    "                stream_callback=callback)\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "# Start the stream\n",
    "stream.start_stream()\n",
    "\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        pass\n",
    "except KeyboardInterrupt:\n",
    "    # Stop the stream if interrupted by the user\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    print(\"Stream stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e4edb-5b1a-402a-a929-cdca474c92cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
