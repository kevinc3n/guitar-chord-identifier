{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae462d7-f525-40d8-aee9-5bbf68a3a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'am' 'bm' 'c' 'd' 'dm' 'e' 'em' 'f' 'g']\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "For audio file recording.wav, predicted class: c\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "For audio file recording.wav, predicted class: d\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "For audio file recording.wav, predicted class: am\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "For audio file recording.wav, predicted class: g\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "For audio file recording.wav, predicted class: am\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "For audio file recording.wav, predicted class: a\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "For audio file recording.wav, predicted class: d\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
      "For audio file recording.wav, predicted class: d\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "For audio file recording.wav, predicted class: d\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "For audio file recording.wav, predicted class: dm\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "For audio file recording.wav, predicted class: bm\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "For audio file recording.wav, predicted class: g\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "For audio file recording.wav, predicted class: am\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "For audio file recording.wav, predicted class: a\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "For audio file recording.wav, predicted class: a\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "For audio file recording.wav, predicted class: am\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "For audio file recording.wav, predicted class: e\n",
      "Listening for threshold level...\n",
      "Threshold level reached. Recording started...\n",
      "Recording stopped...\n",
      "Recording saved as recording.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "For audio file recording.wav, predicted class: em\n",
      "Listening for threshold level...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 162\u001b[0m\n\u001b[0;32m    159\u001b[0m     recorder\u001b[38;5;241m.\u001b[39mstart_recording()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 162\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[7], line 159\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    156\u001b[0m recorder \u001b[38;5;241m=\u001b[39m AudioRecorder(filename)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Start listening and recording continuously\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m recorder\u001b[38;5;241m.\u001b[39mstart_recording()\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36mAudioRecorder.start_recording\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mListening for threshold level...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreshold level reached. Recording started...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mread_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream, num_frames,\n\u001b[0;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import sys\n",
    "import threading\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Global variable for the model\n",
    "model = load_model('cnn_attempt_98_acc.h5')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Given list of strings\n",
    "chords_list = ['a', 'am', 'bm', 'c', 'd', 'dm', 'e', 'em', 'f', 'g']\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "chords_array = np.array(chords_list)\n",
    "\n",
    "y = label_encoder.fit_transform(chords_array)\n",
    "\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self, filename='recording.wav', chunk=1024, channels=1, rate=44100, threshold=2000, duration=2):\n",
    "        self.filename = filename\n",
    "        self.chunk = chunk\n",
    "        self.channels = channels\n",
    "        self.rate = rate\n",
    "        self.frames = []\n",
    "        self.threshold = threshold\n",
    "        self.duration = duration\n",
    "        self.pa = pyaudio.PyAudio()\n",
    "        self.stream = self.pa.open(format=pyaudio.paInt16,\n",
    "                                   channels=self.channels,\n",
    "                                   rate=self.rate,\n",
    "                                   input=True,\n",
    "                                   frames_per_buffer=self.chunk)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def start_recording(self):\n",
    "        while True:\n",
    "            print(\"Listening for threshold level...\")\n",
    "            while True:\n",
    "                data = np.frombuffer(self.stream.read(self.chunk), dtype=np.int16)\n",
    "                if np.max(data) > self.threshold:\n",
    "                    print(\"Threshold level reached. Recording started...\")\n",
    "                    self.record()\n",
    "                    break\n",
    "\n",
    "    def record(self):\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < self.duration:\n",
    "            data = self.stream.read(self.chunk)\n",
    "            self.lock.acquire()\n",
    "            self.frames.append(data)\n",
    "            self.lock.release()\n",
    "        self.stop_recording()\n",
    "        self.frames.clear()\n",
    "        time.sleep(0.5)  # Wait for a moment before restarting recording\n",
    "\n",
    "    def stop_recording(self):\n",
    "        print(\"Recording stopped...\")\n",
    "        with wave.open(self.filename, 'wb') as wf:\n",
    "            wf.setnchannels(self.channels)\n",
    "            wf.setsampwidth(self.pa.get_sample_size(pyaudio.paInt16))\n",
    "            wf.setframerate(self.rate)\n",
    "            self.lock.acquire()\n",
    "            wf.writeframes(b''.join(self.frames))\n",
    "            self.lock.release()\n",
    "        print(f\"Recording saved as {self.filename}\")\n",
    "        self.predict(self.filename)\n",
    "\n",
    "    def predict(self, filename):\n",
    "        # Load the audio file\n",
    "        audio_file = filename  # Use the recorded WAV file\n",
    "        audio_sample, sr = librosa.load(audio_file)\n",
    "    \n",
    "        # Preprocess the audio to generate features\n",
    "        features = generate_features(audio_sample)\n",
    "    \n",
    "        # Reshape the feature array to match the input shape of the model\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "    \n",
    "        # Make predictions\n",
    "        predictions = model.predict(features)\n",
    "    \n",
    "        # Get the predicted class\n",
    "        predicted_class = np.argmax(predictions)\n",
    "\n",
    "        predicted_categorical_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "        # Print the predicted class\n",
    "        print(f\"For audio file {audio_file}, predicted class: {predicted_categorical_label}\")\n",
    "\n",
    "def padding(array, xx, yy):\n",
    "    \"\"\"\n",
    "    :param array: numpy array\n",
    "    :param xx: desired height\n",
    "    :param yy: desirex width\n",
    "    :return: padded array\n",
    "    \"\"\"\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "    a = max((xx - h) // 2,0)\n",
    "    aa = max(0,xx - a - h)\n",
    "    b = max(0,(yy - w) // 2)\n",
    "    bb = max(yy - b - w,0)\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')\n",
    "\n",
    "def normalize(feature):\n",
    "    return (feature - np.min(feature)) / (np.max(feature) - np.min(feature))\n",
    "\n",
    "def generate_features(audio_sample):\n",
    "    max_size = 1000  # Define your max audio feature width\n",
    "    n_mfcc = 13  # Number of MFCC coefficients\n",
    "    sr = 22050\n",
    "\n",
    "    # Extract features\n",
    "    stft = librosa.stft(y=audio_sample, n_fft=255, hop_length=512)\n",
    "    stft = stft[:, :max_size]  # Truncate stft to max_size\n",
    "    stft = padding(np.abs(stft), 128, max_size)\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_sample, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs = mfccs[:, :max_size]  # Truncate mfccs to max_size\n",
    "    mfccs = padding(mfccs, 128, max_size)\n",
    "\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=audio_sample, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=audio_sample, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=audio_sample, sr=sr)\n",
    "\n",
    "    spec_bw_truncated = spec_bw[:, :max_size]  # Truncate spec_bw to max_size\n",
    "    spec_centroid_truncated = spec_centroid[:, :max_size]  # Truncate spec_centroid to max_size\n",
    "    chroma_stft_truncated = chroma_stft[:, :max_size]\n",
    "\n",
    "    # Create the image stack\n",
    "    image = np.array([padding(normalize(spec_bw_truncated), 1, max_size)]).reshape(1, max_size)\n",
    "    image = np.append(image, padding(normalize(spec_centroid_truncated), 1, max_size), axis=0)\n",
    "\n",
    "    for i in range(0, 9):\n",
    "        image = np.append(image, padding(normalize(spec_bw_truncated), 1, max_size), axis=0)\n",
    "        image = np.append(image, padding(normalize(spec_centroid_truncated), 1, max_size), axis=0)\n",
    "        image = np.append(image, padding(normalize(chroma_stft_truncated), 12, max_size), axis=0)\n",
    "\n",
    "    # Stack STFT and MFCCs\n",
    "    image = np.dstack((image, stft))\n",
    "    image = np.dstack((image, mfccs))\n",
    "\n",
    "    return image\n",
    "\n",
    "def main():\n",
    "    filename = 'recording.wav'\n",
    "    recorder = AudioRecorder(filename)\n",
    "\n",
    "    # Start listening and recording continuously\n",
    "    recorder.start_recording()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61995213-8209-4e70-86b5-037701c5d464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b444e64-57ce-4022-be0e-dec27f7f0623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
